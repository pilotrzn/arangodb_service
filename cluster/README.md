# Кластер

## Описание

Кластерная архитектура ArangoDB представляет собой модель CP master/master без единой точки отказа.

С помощью “CP” в терминах теоремы CAP мы подразумеваем, что при наличии сетевого раздела база данных предпочитает внутреннюю согласованность доступности. Под “master/master” мы подразумеваем, что клиенты могут отправлять свои запросы на произвольный узел и получать одинаковое представление в базе данных независимо. “Нет единой точки отказа” означает, что кластер может продолжать обслуживать запросы, даже если один компьютер полностью выходит из строя.

Таким образом, ArangoDB был разработан как распределенная база данных с несколькими моделями. В этом разделе дается краткое описание архитектуры кластера и того, как достигаются вышеуказанные функции и возможности.

Кластер ArangoDB состоит из нескольких экземпляров ArangoDB, которые взаимодействуют друг с другом по сети. Они играют разные роли, которые подробно описаны ниже.

Текущая конфигурация кластера хранится в агентстве, которое представляет собой высокодоступное устойчивое хранилище ключей/значений, основанное на нечетном количестве экземпляров ArangoDB, использующих протокол консенсуса Raft.

## Роли

Для различных экземпляров в кластере ArangoDB существуют три различные роли:

- Агенты
- Координаторы
- DB-серверы.

![topology](cluster-topology.png)


### Agency

Один или несколько агентов образуют Agency в кластере ArangoDB. Agency является центральным местом для хранения конфигурации в кластере. Он выполняет выборы лидера и предоставляет другие службы синхронизации для всего кластера. Без Agency ни один из других компонентов не может работать.

Хотя Agency, как правило, невидимо для посторонних, оно является сердцем Кластера. Таким образом, отказоустойчивость, конечно, обязательна для Agency. Для достижения этого агенты используют алгоритм консенсуса Raft. Алгоритм формально гарантирует бесконфликтное управление конфигурацией в кластере ArangoDB.

По своей сути Agency управляет большим деревом конфигурации. Он поддерживает транзакционные операции чтения и записи в этом дереве, а другие серверы могут подписаться на обратные вызовы HTTP для всех изменений в дереве.

### Coordinators

Координаторы должны быть доступны извне. Это те, с которыми общаются клиенты. Они координируют задачи кластера, такие как выполнение запросов и запуск служб Foxx. Они знают, где хранятся данные, и оптимизируют, где запускать пользовательские запросы или их части. Координаторы не имеют состояния и, таким образом, могут быть легко отключены и перезапущены по мере необходимости.

### DB-servers

DB-серверы - это те, на которых фактически размещаются данные. В них размещаются фрагменты данных, и, используя синхронную репликацию, DB-сервер может быть либо лидером, либо последователем для фрагмента. Операции с документами сначала применяются к лидеру, а затем синхронно реплицируются на всех последователей.

Доступ к сегментам должен осуществляться не извне, а косвенно через Координаторов. Они также могут выполнять запросы частично или целиком по запросу Координатора.

## Варианты конфигураций

Эта архитектура очень гибкая и, следовательно, допускает множество конфигураций, подходящих для различных сценариев использования:

- Конфигурация по умолчанию заключается в запуске ровно одного координатора и одного DB-сервера на каждой машине. Это обеспечивает классическую настройку master/master, поскольку между различными узлами существует идеальная симметрия, клиенты могут одинаково хорошо общаться с любым из координаторов, и все они предоставляют одно и то же представление в хранилище данных. Агенты могут запускаться на отдельных, менее мощных машинах.

- Можно развернуть больше координаторов, чем DB-серверов. Это разумный подход, если для служб Foxx требуется большая мощность процессора, потому что они работают на Координаторах.

- Можно развернуть больше серверов БД, чем координаторов, если требуется больший объем данных, а производительность запросов является меньшим узким местом.

- Можно развернуть координатора на каждой машине сервера приложений (например, node.js сервер), а агенты и DB-серверы на отдельном наборе машин. Это позволяет избежать сетевого перехода между сервером приложений и базой данных и, таким образом, уменьшает задержку. По сути, это перемещает часть логики распространения базы данных на компьютер, на котором выполняется клиент.

Best-practice: экземпляры агентов и экземпляры БД на разных машинах

!!! Предполагается, что различные экземпляры, образующие кластер, должны выполняться в одном центре обработки данных (DC) с надежным и высокоскоростным сетевым подключением между всеми машинами, участвующими в кластере.

- [Предварительная информация](preliminary_info.md)

Инструменты развертывания:

- [Вручную](manual.md)

- [ArangoDB Starter](starter.md)

- [Kubernetes](../kubernetes/README.md)


 [Назад](../README.md)